[{"content":" Consider a problem that we have an ellipse described by a polynomial equation given by\n\\[\\begin{equation} x^{T}Ax+B^{T}x+f=0 \\end{equation}\\] where \\(x,B\\in\\mathbb{R}^{2}\\), \\(A\\) is a 2 by 2 real symmetric matrix, and \\(f\\in\\mathbb{R}\\).\nTo find points on the curve, one may first needs to find out the range the ellipse in. For example, \\(a\\leq x_{1}\\leq b\\). And then solving a second-order equation.\nThat is, setting one variable as a parameter and making the other one be changed according to the equation. This approach directly deals with the coordinates in the space, which may not be the most simple and efficient one to find points on the ellipse. As an alternative, we can consider the ellipse as an affine transformation of a circle. We try to find a transformation that \\(x=St+t_{0}\\) so that the equation would be\n\\[\\begin{equation} t^{T}t=1. \\end{equation}\\] By substitution, the equation can be\n\\[t^{T}S^{T}ASt+\\left(B^{T}S+2t_{0}^{T}AS\\right)t=-B^{T}t_{0}-t_{0}^{T}At_{0}-f\\] Let’s setup some conditions:\n\\[\\begin{cases} S^{T}AS \u0026amp; =r^{2}\\\\ B^{T}S+2t_{0}^{T}AS \u0026amp; =0\\\\ -B^{T}t_{0}-t_{0}^{T}At_{0}-f \u0026amp; =r^{2}. \\end{cases}\\] If these conditions are satisfied, the equation would be \\(t^{T}r^{2}t+0t=r^{2}\\), which is equivelently, \\(t^{T}t=1\\).\nNow we need to find \\(S\\) that satisfies the condition. Since A is a real symmetric matrix, it can be diagonized into a form of \\(A=Q\\Lambda Q^{T}\\) where \\(Q^{-1}=Q^{T}\\). By setting \\(S=rQ\\Lambda^{-\\frac{1}{2}}\\), it can be shown that\n\\[S^{T}AS=r\\Lambda^{-\\frac{1}{2}}Q^{T}ArQ\\Lambda^{-\\frac{1}{2}}=r^{2}\\Lambda^{-\\frac{1}{2}}Q^{T}Q\\Lambda Q^{T}Q\\Lambda^{-\\frac{1}{2}}=r^{2}.\\] Note that to make \\(\\Lambda^{-\\frac{1}{2}}\\) exist, all the eigenvalues of A must be positve so that they have real square root.\nFrom the second condition, we can solve for \\(t_{0}\\)\n\\[\\begin{aligned} B^{T}S+2t_{0}^{T}AS\u0026amp;=B^{T}rQ\\Lambda^{-\\frac{1}{2}}+2t_{0}^{T}Q\\Lambda Q^{T}rQ\\Lambda^{-\\frac{1}{2}}\\\\\u0026amp;=B^{T}Q\\Lambda^{-\\frac{1}{2}}+2t_{0}^{T}Q\\Lambda^{\\frac{1}{2}}\\\\\u0026amp;=0 \\end{aligned}\\] \\[2t_{0}^{T}Q\\Lambda Q^{T}=-B^{T}\\] \\[2At_{0}=-B\\] From the third condition, we can simply solve for \\(r=\\sqrt{-B^{T}t_{0}-t_{0}^{T}At_{0}-f}\\).\nTo sum up, an equation of the form of\n\\[x^{T}Ax+B^{T}x+f=0.\\] If we can make sure this is an ellipse, then we can find a tranformation of \\(x=St+t_{0}\\)\nwhere\n\\[\\begin{cases} S \u0026amp; =rQ\\Lambda^{-\\frac{1}{2}}\\\\ A \u0026amp; =Q\\Lambda Q^{T}\\\\ 2At_{0} \u0026amp; =-B\\\\ r \u0026amp; =\\sqrt{-B^{T}t_{0}-t_{0}^{T}At_{0}-f} \\end{cases},\\] which make the ellipse in \\(x\\) a circle in \\(t\\). That is, \\(t^{T}t=1\\) is satisfied.\n","permalink":"https://fulva1230.github.io/daggy/posts/parameterized_ellipse/","summary":"Consider a problem that we have an ellipse described by a polynomial equation given by\n\\[\\begin{equation} x^{T}Ax+B^{T}x+f=0 \\end{equation}\\] where \\(x,B\\in\\mathbb{R}^{2}\\), \\(A\\) is a 2 by 2 real symmetric matrix, and \\(f\\in\\mathbb{R}\\).\nTo find points on the curve, one may first needs to find out the range the ellipse in. For example, \\(a\\leq x_{1}\\leq b\\). And then solving a second-order equation.\nThat is, setting one variable as a parameter and making the other one be changed according to the equation.","title":"Parameterize Ellipse"},{"content":" How to solve \\(\\frac{dx^x}{dx}\\)?\nObserve that the expression is an exponential function whoise base and exponent both are a function of \\(x\\). When both the base and exponent change with \\(x\\), how the expression changes with \\(x\\) is quite complicated. To simplify, some techniques can apply to fix part of an expression when taking its derivative.\nDefine a multivariate function with two independent variables \\(s\\) and \\(t\\):\n\\[\\begin{equation} z = s^t \\end{equation}\\] where either the \\(s\\) and \\(t\\) are a function of \\(x\\). Simply put, \\(z\\) is a function of \\(x\\). Take derivative of \\(z\\) involves taking partial derivatives about \\(s\\) and \\(t\\). That is,\n\\[\\begin{equation} \\frac{dz}{dx}=\\frac{ds}{dx}\\frac{\\partial z}{\\partial s}+\\frac{dt}{dx}\\frac{\\partial z}{\\partial t}. \\end{equation}\\] When taking partial derivatives, the other independent variables are considered fixed, so the partial derivatives are\n\\[\\begin{equation} \\begin{cases} \\frac{\\partial z}{\\partial s} \u0026amp; =ts^{t-1}\\\\ \\frac{\\partial z}{\\partial t} \u0026amp; =s^{t}\\ln s. \\end{cases} \\end{equation}\\] To solve \\(\\frac{dx^x}{dx}\\), substitute \\(s\\) and \\(t\\) with \\(x\\) and apply the equations above, which yields\n\\[\\begin{equation} \\begin{aligned}\\frac{dx^{x}}{dx} \u0026amp; =\\frac{dz}{dx}|_{s,t=x}\\\\ \u0026amp; =\\left(ts^{t-1}+s^{t}\\ln s\\right)|_{s,t=x}\\\\ \u0026amp; =x\\cdot x^{x-1}+x^{x}\\ln x\\\\ \u0026amp; =\\left(1+\\ln x\\right)x^{x}, \\end{aligned} \\end{equation}\\] which is the answer of the problem.\n","permalink":"https://fulva1230.github.io/daggy/posts/solve-x-square-x/","summary":"How to solve \\(\\frac{dx^x}{dx}\\)?\nObserve that the expression is an exponential function whoise base and exponent both are a function of \\(x\\). When both the base and exponent change with \\(x\\), how the expression changes with \\(x\\) is quite complicated. To simplify, some techniques can apply to fix part of an expression when taking its derivative.\nDefine a multivariate function with two independent variables \\(s\\) and \\(t\\):\n\\[\\begin{equation} z = s^t \\end{equation}\\] where either the \\(s\\) and \\(t\\) are a function of \\(x\\).","title":"Solve the Derivative of $x^x$"},{"content":" Problem Let \\(X\\) be the number of 1’s and \\(Y\\) the number of 2’s that occur in stem[n] rolls of a fair die. Compute \\(\\text{Cov}(X,Y)\\).\nSolution To solve for \\(\\text{Cov}(X,Y)\\), we need to first find the joint probability mass function of \\(X\\) and \\(Y\\). Consider that the event where \\(X+Y\u0026gt;n\\); The event would never happen. For the event \\(B\\) where \\(X+Y \\leq n\\), to know the probability, let us consider a specific event \\(A\\): First \\(X = x\\) rolls result in 1’s. The following \\(Y = y\\) rolls result in 2’s. All the remaining \\(n-x-y\\) result in neither 1’s nor 2’s. The event has a probability given by\n\\[\\begin{equation} P\\left[A\\right]=\\left(\\frac{1}{6}\\right)^{x}\\left(\\frac{1}{6}\\right)^{y}\\left(\\frac{5}{6}\\right)^{n-x-y}. \\label{eq:probability-of-A} \\end{equation}\\] The event \\(A\\) is a subset of the event \\(B\\). That is, an outcome of the event \\(A\\) is also an outcome of the event \\(B\\). To know the probability of the event \\(B\\), we need to apply \\eqref{eq:probability-of-A} to find \\(P[B]\\). The event \\(A\\) actually is a specific permutation of the event \\(B\\). To find \\(P[B]\\), we need to find the number of the permutations of the event \\(B\\).\nTo find the number of the permutations, we need to find a task whois outcomes can represent the permutations. Consider that there are \\(n\\) distinct items to be divided into 3 groups. One group has \\(x\\) items. Another one has \\(y\\) items. The other one has \\(n-x-y\\) items. The number of the outcomes is given by\n\\[\\begin{equation} \\frac{n!}{x!y!\\left(n-x-y\\right)!}\\label{eq:permutations} \\end{equation}\\] The number of the outcomes can tell us the number of the permutations. Consider that the items have numbers from 1 to \\(n\\) and the groups have labels of \u0026#34;1’s\u0026#34;, \u0026#34;2’s\u0026#34;, and \u0026#34;otherwise\u0026#34;. When the item with number \\(k\\) is assigned to group with label \u0026#34;1’s\u0026#34;, it means that the \\(k\\)-th roll gives 1. By such interpretation of the outcomes, we can find that there is one-to-one correspondence the outcomes and the permutations.\nWith the number of the outcomes, the joint pmf of \\(X\\) and \\(Y\\) is given by\n\\[\\begin{equation} p\\left(x,y\\right)=\\begin{cases} \\frac{n!}{x!y!\\left(n-x-y\\right)!}\\left(\\frac{1}{6}\\right)^{x+y}\\left(\\frac{5}{6}\\right)^{n-x-y} \u0026amp; \\text{if }x+y\\leq n\\text{ and }x,y\\geq0\\\\ 0 \u0026amp; \\text{otherwise} \\end{cases}\\label{eq:joint_probability} \\end{equation}\\] We can now focus on solving for \\(\\text{Cov}(X,Y)\\). Expanding it yields\n\\[\\begin{align} \\text{Cov}\\left(X,Y\\right) \u0026amp; =E\\left[\\left(X-E\\left[X\\right]\\right)\\left(Y-E\\left[Y\\right]\\right)\\right]\\nonumber \\\\ \u0026amp; =E\\left[XY\\right]-E\\left[X\\right]E\\left[Y\\right].\\label{eq:cov_XY} \\end{align}\\] We need to find \\(E[XY]\\), \\(E[X]\\), and \\(E[Y]\\). Since each roll has \\(\\frac{1}{6}\\) chance to yield 1, it is simply that\n\\[\\begin{equation} E\\left[X\\right]=E\\left[Y\\right]=\\frac{n}{6}\\label{eq:E_X_or_Y} \\end{equation}\\] To find \\(E[XY]\\), we need to expand it,\n\\[\\begin{align} E\\left[XY\\right] \u0026amp; =\\sum_{i=0}^{n}\\sum_{j=0}^{n-i}p\\left(i,j\\right)ij\\nonumber \\\\ \u0026amp; =\\sum_{i=1}^{n-1}\\sum_{j=1}^{n-i}\\frac{n!ij}{i!j!\\left(n-i-j\\right)!}\\left(\\frac{1}{6}\\right)^{i+j}\\left(\\frac{5}{6}\\right)^{n-i-j}\\label{eq:E_XY}\\\\ \u0026amp; =\\sum_{i=1}^{n-1}\\sum_{j=1}^{n-i}\\frac{n!}{\\left(i-1\\right)!\\left(j-1\\right)!\\left(n-i-j\\right)!}\\left(\\frac{1}{6}\\right)^{i+j}\\left(\\frac{5}{6}\\right)^{n-i-j}\\nonumber \\end{align}\\] The general idea to solve for \\eqref{eq:E_XY} is using the fact that a pmf sums to 1. Consider that\n\\[\\begin{align*} E\\left[XY\\right] \u0026amp; =\\sum_{i=1}^{n}\\sum_{j=1}^{n-i}\\frac{n!}{\\left(i-1\\right)!\\left(j-1\\right)!\\left(n-i-j\\right)!}\\left(\\frac{1}{6}\\right)^{i+j}\\left(\\frac{5}{6}\\right)^{n-i-j}\\\\ \u0026amp; =\\sum_{i=1}^{n-1}\\sum_{j=1}^{n-i}\\frac{n!}{\\left(i-1\\right)!\\left(j-1\\right)!\\left(n-i-j\\right)!}\\left(\\frac{1}{6}\\right)^{i+j}\\left(\\frac{5}{6}\\right)^{n-i-j}\\\\ \u0026amp; =\\sum_{i=0}^{n-2}\\sum_{j=1}^{n-1-i}\\frac{n!}{\\left(i\\right)!\\left(j-1\\right)!\\left(n-1-i-j\\right)!}\\left(\\frac{1}{6}\\right)^{i+1+j}\\left(\\frac{5}{6}\\right)^{n-1-i-j}\\\\ \u0026amp; =\\sum_{i=0}^{n-2}\\sum_{j=0}^{n-2-i}\\frac{n!}{\\left(i\\right)!\\left(j\\right)!\\left(n-2-i-j\\right)!}\\left(\\frac{1}{6}\\right)^{i+j+2}\\left(\\frac{5}{6}\\right)^{n-2-i-j}\\\\ \u0026amp; =\\frac{n\\left(n-1\\right)}{36}\\sum_{i=0}^{n-2}\\sum_{j=0}^{n-2-i}\\frac{\\left(n-2\\right)!}{\\left(i\\right)!\\left(j\\right)!\\left(n-2-i-j\\right)!}\\left(\\frac{1}{6}\\right)^{i+j}\\left(\\frac{5}{6}\\right)^{n-2-i-j}\\\\ \u0026amp; =\\frac{n\\left(n-1\\right)}{36}. \\end{align*}\\] Thus, we have\n\\[\\begin{equation} E\\left[XY\\right]=\\frac{n\\left(n-1\\right)}{36}\\label{eq:E_XY-1}. \\end{equation}\\] By applying \\eqref{eq:cov_XY}, \\eqref{eq:E_X_or_Y}, and \\eqref{eq:E_XY-1}, \\(\\text{Cov}(X,Y)\\) is given by\n\\[\\text{Cov}\\left(X,Y\\right)=E\\left[XY\\right]-E\\left[X\\right]E\\left[Y\\right]=\\frac{n\\left(n-1\\right)}{36}-\\frac{n^{2}}{36}=-\\frac{n}{36}.\\] ","permalink":"https://fulva1230.github.io/daggy/posts/solve-ross-probability-7-36/","summary":"Problem Let \\(X\\) be the number of 1’s and \\(Y\\) the number of 2’s that occur in stem[n] rolls of a fair die. Compute \\(\\text{Cov}(X,Y)\\).\nSolution To solve for \\(\\text{Cov}(X,Y)\\), we need to first find the joint probability mass function of \\(X\\) and \\(Y\\). Consider that the event where \\(X+Y\u0026gt;n\\); The event would never happen. For the event \\(B\\) where \\(X+Y \\leq n\\), to know the probability, let us consider a specific event \\(A\\): First \\(X = x\\) rolls result in 1’s.","title":"Solve Ross's Probability CH.7 Q.36"},{"content":" 我們能無緣無故的傷害他人嗎？如果不行，那理由是什麼？從國小、國中到高中，校園霸凌一直在發生。許多的影視作品、小說、漫畫也多圍繞在校園霸凌上。好像人天生就有個傾向，喜歡創造受害者。即便受害者從未做任何事招惹那些霸凌者，卻依然會在機緣下被相中，而成為受霸凌的對象。想像一下，你面對一位霸凌者，你要告訴他，霸凌是不好的，並用充分的理由說服他。要怎麼說，才能說服眼前只在乎自己利益的本位主義者，無緣無故傷害他人關乎自己的利益，應當避免之呢？\n有人可能表示，傷害別人可能會引起報復，要避免被報復的方法，就是盡可能避免無謂的傷害他人。這樣推論有其道理，但卻無法讓「避免無謂傷害他人」成為一個通用的準則。在這個社會上，有社經地位高的人，也有身體結實的人，但也不乏沒有權勢，沒有背景跟能力的人。弱小的人沒有辦法保護自己，遑論報復他人。傷害弱小的人，並不會被報復，所以一個人如果因怕被報復，而不去傷害他人，那麼他就可以肆無忌憚的傷害弱小的人。\n雖然理想上傷害弱小的人，不須擔心被報復，但是在實務上，要判斷別人比自己弱小並非容易的事。有時，會咬人的狗不會叫，看似膽小怯懦的人，在逼近絕境時，也可能會採取極端的作為。從一個人的外在行為，實在很難預測他在受傷害後，是否會報復。當然，如果有一個儀器，當那儀器掃描一個人後，就能立刻知道結果，那事情會簡單的多。但現實情形是，這樣的儀器並不存在。\n一個人也有可能忘記評估他人是否弱小，因而招惹到錯誤的人。畢竟，人是習慣的動物，在處理事情上，人往往不是深思熟慮後做出決定，而是選擇依循舊習。過去怎麼做，將來也就那麼做。如果一個人過去就愛霸凌別人，他未來勢必也會找人霸凌。當他養成了這個習慣，這個習慣就可能會蒙蔽他的雙眼，讓他忘記霸凌別人前應當謹慎評估。\n所以說在實務上，最好避免不被報復的方法，就是避免無緣無故的傷害任何人，無論對象是手無縛雞之力，亦或是膽小如鼠。因為從一個人的行為舉止，外貿能做出的判斷有限，要謹慎的篩選受害者太難了。更甚，當傷害他人成為習慣，會讓一個人以為身邊每個人都那麼「好欺負」，而惹錯對象。綜上所述，雖然理想上本位主義者「可以」無緣無故傷害他人，但在實務上，他最好還是安分守己，才是最符合他的利益。\n","permalink":"https://fulva1230.github.io/daggy/posts/can-we-harm-others/","summary":"我們能無緣無故的傷害他人嗎？如果不行，那理由是什麼？從國小、國中到高中，校園霸凌一直在發生。許多的影視作品、小說、漫畫也多圍繞在校園霸凌上。好像人天生就有個傾向，喜歡創造受害者。即便受害者從未做任何事招惹那些霸凌者，卻依然會在機緣下被相中，而成為受霸凌的對象。想像一下，你面對一位霸凌者，你要告訴他，霸凌是不好的，並用充分的理由說服他。要怎麼說，才能說服眼前只在乎自己利益的本位主義者，無緣無故傷害他人關乎自己的利益，應當避免之呢？\n有人可能表示，傷害別人可能會引起報復，要避免被報復的方法，就是盡可能避免無謂的傷害他人。這樣推論有其道理，但卻無法讓「避免無謂傷害他人」成為一個通用的準則。在這個社會上，有社經地位高的人，也有身體結實的人，但也不乏沒有權勢，沒有背景跟能力的人。弱小的人沒有辦法保護自己，遑論報復他人。傷害弱小的人，並不會被報復，所以一個人如果因怕被報復，而不去傷害他人，那麼他就可以肆無忌憚的傷害弱小的人。\n雖然理想上傷害弱小的人，不須擔心被報復，但是在實務上，要判斷別人比自己弱小並非容易的事。有時，會咬人的狗不會叫，看似膽小怯懦的人，在逼近絕境時，也可能會採取極端的作為。從一個人的外在行為，實在很難預測他在受傷害後，是否會報復。當然，如果有一個儀器，當那儀器掃描一個人後，就能立刻知道結果，那事情會簡單的多。但現實情形是，這樣的儀器並不存在。\n一個人也有可能忘記評估他人是否弱小，因而招惹到錯誤的人。畢竟，人是習慣的動物，在處理事情上，人往往不是深思熟慮後做出決定，而是選擇依循舊習。過去怎麼做，將來也就那麼做。如果一個人過去就愛霸凌別人，他未來勢必也會找人霸凌。當他養成了這個習慣，這個習慣就可能會蒙蔽他的雙眼，讓他忘記霸凌別人前應當謹慎評估。\n所以說在實務上，最好避免不被報復的方法，就是避免無緣無故的傷害任何人，無論對象是手無縛雞之力，亦或是膽小如鼠。因為從一個人的行為舉止，外貿能做出的判斷有限，要謹慎的篩選受害者太難了。更甚，當傷害他人成為習慣，會讓一個人以為身邊每個人都那麼「好欺負」，而惹錯對象。綜上所述，雖然理想上本位主義者「可以」無緣無故傷害他人，但在實務上，他最好還是安分守己，才是最符合他的利益。","title":"我們可以傷害他人嗎?"},{"content":"Question If $X_{1},X_{2},\u0026hellip;X_{n}$ are independent and identically distributed random variables having uniform distributions over $(0,1)$, find\n(a). $E\\left[\\max\\left(X_{1},\u0026hellip;,X_{n}\\right)\\right]$;\n(b). $E\\left[\\min\\left(X_{1},\u0026hellip;,X_{n}\\right)\\right]$\nAnswer a. The joint probability density function is\n$$ f_{X_{1},X_{2},...,X_{n}}\\left(x_{1},x_{2},...,x_{n}\\right)=\\begin{cases} 1 \u0026 \\text{if }x_{1},x_{2},...,x_{n}\\in[0,1]\\\\ 0 \u0026 \\text{otherwise}. \\end{cases} $$ The re-ordered joint probability density function is $$ f_{\\hat{X}_{1},\\hat{X}_{2},...,\\hat{X}_{n}}\\left(x_{1},x_{2},...,x_{n}\\right)=\\begin{cases} n! \u0026amp; \\text{if }0\u0026lt;x_{1}\u0026lt;x_{2}\u0026lt;,...,\u0026lt;x_{n}\u0026lt;1\\\\ 0 \u0026amp; \\text{otherwise} \\end{cases} \\qquad(1)$$ We can make use of the new pdf given in Equation 1, considering\n$$ \\begin{align*} E\\left[\\max\\left(X_{1},...,X_{n}\\right)\\right] \u0026 =E\\left[\\hat{X}_{n}\\right]\\\\ \u0026 =\\int_{0}^{1}\\int_{0}^{x_{n}}\\int_{0}^{x_{n-1}}...\\int_{0}^{x_{2}}x_{n}f_{\\hat{X}_{1},\\hat{X}_{2},...,\\hat{X}_{n}}\\left(x_{1},x_{2},...,x_{n}\\right)dx_{1}...dx_{n-2}dx_{n-1}dx_{n}\\\\ \u0026 =\\int_{0}^{1}\\int_{0}^{x_{n}}\\int_{0}^{x_{n-1}}...\\int_{0}^{x_{2}}x_{n}n!dx_{1}...dx_{n-2}dx_{n-1}dx_{n}\\\\ \u0026 =\\int_{0}^{1}x_{n}n!\\frac{x_{n}^{n-1}}{\\left(n-1\\right)!}dx_{n}=\\int_{0}^{1}nx_{n}^{n}dx_{n}=\\frac{n}{n+1} \\end{align*} $$ Then we find the answer.\nb. For the expection of the minimum, considering\n$$ \\begin{align*} E\\left[\\min\\left(X_{1},...,X_{n}\\right)\\right] \u0026 =E\\left[\\hat{X}_{1}\\right]\\\\ \u0026 =\\int_{0}^{1}\\int_{0}^{x_{n}}\\int_{0}^{x_{n-1}}...\\int_{0}^{x_{2}}x_{1}f_{\\hat{X}_{1},\\hat{X}_{2},...,\\hat{X}_{n}}\\left(x_{1},x_{2},...,x_{n}\\right)dx_{1}...dx_{n-2}dx_{n-1}dx_{n}\\\\ \u0026 =n!\\int_{0}^{1}\\int_{0}^{x_{n}}\\int_{0}^{x_{n-1}}...\\int_{0}^{x_{2}}x_{1}dx_{1}...dx_{n-2}dx_{n-1}dx_{n}\\\\ \u0026 =n!\\frac{1}{\\left(n+1\\right)!}\\\\ \u0026 =\\frac{1}{n+1} \\end{align*} $$ ","permalink":"https://fulva1230.github.io/daggy/posts/max-min-expectation/","summary":"Question If $X_{1},X_{2},\u0026hellip;X_{n}$ are independent and identically distributed random variables having uniform distributions over $(0,1)$, find\n(a). $E\\left[\\max\\left(X_{1},\u0026hellip;,X_{n}\\right)\\right]$;\n(b). $E\\left[\\min\\left(X_{1},\u0026hellip;,X_{n}\\right)\\right]$\nAnswer a. The joint probability density function is\n$$ f_{X_{1},X_{2},...,X_{n}}\\left(x_{1},x_{2},...,x_{n}\\right)=\\begin{cases} 1 \u0026 \\text{if }x_{1},x_{2},...,x_{n}\\in[0,1]\\\\ 0 \u0026 \\text{otherwise}. \\end{cases} $$ The re-ordered joint probability density function is $$ f_{\\hat{X}_{1},\\hat{X}_{2},...,\\hat{X}_{n}}\\left(x_{1},x_{2},...,x_{n}\\right)=\\begin{cases} n! \u0026amp; \\text{if }0\u0026lt;x_{1}\u0026lt;x_{2}\u0026lt;,...,\u0026lt;x_{n}\u0026lt;1\\\\ 0 \u0026amp; \\text{otherwise} \\end{cases} \\qquad(1)$$ We can make use of the new pdf given in Equation 1, considering","title":"Max Min Expectation"},{"content":"問題描述 N個人參加一個晚宴，他們依隨機的順序分別抵達晚宴會場，他們任一人到達晚宴時，會尋找他們是否有朋友已待在晚宴裡，如果有，就會與朋友共桌，如果沒有，則會自己坐一張還未有人佔的桌子邊。假設在這N個人中隨機挑兩人，會有p的機率兩人是朋友。問在N個人抵達會場後，會場被他們佔用的桌子數的期望值是多少。1\n解 這個問題只需要知道桌子數的期望值，所以比起用占用的桌子數當作隨機變數，可以用虛擬變量(Dummy variable) \\(X_i\\) ，定義為第 \\(i\\) 個抵達的人是否開新桌，是的話為1，不是則為0。因為第 \\(i\\) 個人要開新桌的條件為第 \\(1..i-1\\) 個人皆不是他朋友，它的Probability mass function就是 \\[ p_{X_{i}}\\left(x\\right)=\\begin{cases} \\left(1-p\\right)^{i-1} \u0026 x=1\\\\ 1-\\left(1-p\\right)^{i-1} \u0026 x=0\\\\ 0 \u0026 \\text{otherwise} \\end{cases},\\text{for }i=1,2,...,N \\] 這樣答案就會是 \\begin{align*} E\\left[\\sum_{i=1}^{N}X_{i}\\right] \u0026 =\\sum_{i=1}^{N}E\\left[X_{i}\\right]\\\\ \u0026 =\\sum_{i=1}^{N}\\left(1-p\\right)^{i-1}\\\\ \u0026 =\\frac{1-\\left(1-p\\right)^{N}}{1-\\left(1-p\\right)}\\\\ \u0026 =\\frac{1-\\left(1-p\\right)^{N}}{p} \\end{align*} 來自 Ross, S. M. (2014). A first course in probability. Pearson P.373 Problem 7.8.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://fulva1230.github.io/daggy/posts/how-many-tables/","summary":"問題描述 N個人參加一個晚宴，他們依隨機的順序分別抵達晚宴會場，他們任一人到達晚宴時，會尋找他們是否有朋友已待在晚宴裡，如果有，就會與朋友共桌，如果沒有，則會自己坐一張還未有人佔的桌子邊。假設在這N個人中隨機挑兩人，會有p的機率兩人是朋友。問在N個人抵達會場後，會場被他們佔用的桌子數的期望值是多少。1\n解 這個問題只需要知道桌子數的期望值，所以比起用占用的桌子數當作隨機變數，可以用虛擬變量(Dummy variable) \\(X_i\\) ，定義為第 \\(i\\) 個抵達的人是否開新桌，是的話為1，不是則為0。因為第 \\(i\\) 個人要開新桌的條件為第 \\(1..i-1\\) 個人皆不是他朋友，它的Probability mass function就是 \\[ p_{X_{i}}\\left(x\\right)=\\begin{cases} \\left(1-p\\right)^{i-1} \u0026 x=1\\\\ 1-\\left(1-p\\right)^{i-1} \u0026 x=0\\\\ 0 \u0026 \\text{otherwise} \\end{cases},\\text{for }i=1,2,...,N \\] 這樣答案就會是 \\begin{align*} E\\left[\\sum_{i=1}^{N}X_{i}\\right] \u0026 =\\sum_{i=1}^{N}E\\left[X_{i}\\right]\\\\ \u0026 =\\sum_{i=1}^{N}\\left(1-p\\right)^{i-1}\\\\ \u0026 =\\frac{1-\\left(1-p\\right)^{N}}{1-\\left(1-p\\right)}\\\\ \u0026 =\\frac{1-\\left(1-p\\right)^{N}}{p} \\end{align*} 來自 Ross, S. M. (2014). A first course in probability. Pearson P.373 Problem 7.8.\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"統計問題：會佔幾張桌子？"},{"content":" Consider a binomial distribution \\(X\\) with parameters \\(n\\) and \\(p\\). Its probability mass function is $$ p\\left(x\\right)=\\begin{cases} \\frac{n!}{\\left(n-x\\right)!x!}p^{x}\\left(1-p\\right)^{n-x} \u0026 0\\leq x\\leq n\\\\ 0 \u0026 \\text{otherwise}. \\end{cases} $$ It's expectation is given by $$ \\begin{align*} E\\left[X\\right] \u0026 =\\sum_{x=0}^{n}xp\\left(x\\right)\\\\ \u0026 =\\sum_{x=1}^{n}\\frac{n!x}{\\left(n-x\\right)!x!}p^{x}\\left(1-p\\right)^{n-x}\\\\ \u0026 =\\sum_{x=1}^{n}np\\frac{\\left(n-1\\right)!}{\\left(n-x\\right)!\\left(x-1\\right)!}p^{\\left(x-1\\right)}\\left(1-p\\right)^{n-x}\\\\ \u0026 =np\\sum_{x=1}^{n}\\frac{\\left(n-1\\right)!}{\\left(n-x\\right)!\\left(x-1\\right)!}p^{\\left(x-1\\right)}\\left(1-p\\right)^{n-x}\\\\ \u0026 =np. \\end{align*} $$ Note that if \\(Y\\) is a binomial distribution with parameter \\(n-1\\) and \\(p\\), it has probability mass function as \\[ p_{Y}\\left(y\\right)=\\begin{cases} \\frac{\\left(n-1\\right)!}{\\left(n-y\\right)!y!}p^{y}\\left(1-p\\right)^{\\left(n-1\\right)-y} \u0026 0\\leq y\\leq n-1\\\\ 0 \u0026 \\text{otherwise} \\end{cases} \\] So \\begin{align*} E\\left[X\\right] \u0026 =np\\sum_{y=0}^{n-1}\\frac{\\left(n-1\\right)!}{\\left(n-y\\right)!y!}p^{y}\\left(1-p\\right)^{\\left(n-1\\right)-y}\\\\ \u0026 =np\\sum_{y=0}^{n-1}p_{Y}\\left(y\\right)\\\\ \u0026 =np. \\end{align*} ","permalink":"https://fulva1230.github.io/daggy/posts/expectation-of-binomial-distribution/","summary":"Consider a binomial distribution \\(X\\) with parameters \\(n\\) and \\(p\\). Its probability mass function is $$ p\\left(x\\right)=\\begin{cases} \\frac{n!}{\\left(n-x\\right)!x!}p^{x}\\left(1-p\\right)^{n-x} \u0026 0\\leq x\\leq n\\\\ 0 \u0026 \\text{otherwise}. \\end{cases} $$ It's expectation is given by $$ \\begin{align*} E\\left[X\\right] \u0026 =\\sum_{x=0}^{n}xp\\left(x\\right)\\\\ \u0026 =\\sum_{x=1}^{n}\\frac{n!x}{\\left(n-x\\right)!x!}p^{x}\\left(1-p\\right)^{n-x}\\\\ \u0026 =\\sum_{x=1}^{n}np\\frac{\\left(n-1\\right)!}{\\left(n-x\\right)!\\left(x-1\\right)!}p^{\\left(x-1\\right)}\\left(1-p\\right)^{n-x}\\\\ \u0026 =np\\sum_{x=1}^{n}\\frac{\\left(n-1\\right)!}{\\left(n-x\\right)!\\left(x-1\\right)!}p^{\\left(x-1\\right)}\\left(1-p\\right)^{n-x}\\\\ \u0026 =np. \\end{align*} $$ Note that if \\(Y\\) is a binomial distribution with parameter \\(n-1\\) and \\(p\\), it has probability mass function as \\[ p_{Y}\\left(y\\right)=\\begin{cases} \\frac{\\left(n-1\\right)!}{\\left(n-y\\right)!y!}p^{y}\\left(1-p\\right)^{\\left(n-1\\right)-y} \u0026 0\\leq y\\leq n-1\\\\ 0 \u0026 \\text{otherwise} \\end{cases} \\] So \\begin{align*} E\\left[X\\right] \u0026 =np\\sum_{y=0}^{n-1}\\frac{\\left(n-1\\right)!","title":"Expectation of Binomial Distribution"},{"content":"在射擊遊戲中，當你奮力殺敵的時候，敵人痛苦的倒下，你是否想過一個問題，當他們倒下時，會不會痛？聽起來是個很奇怪的問題對吧，你肯定認為敵人會痛苦的倒下，是因為有人寫了程式，計算敵人受到的傷害，當生命值歸零的那個瞬間，便播放痛苦倒下的動畫，當然，動畫是人預先畫出來的。\n因為程式內部的計算是生硬的生命值加減，所以在遊戲中敵人痛苦的倒下只是個假象，實際上沒有一個意識，會經歷痛苦，並隨著你殺敵而消逝。但當你看著某個活生生的人，不管為了什麼原因，露出痛苦的表情時，你卻不會告訴自己說：「那個人只是因為大腦內電磁波、神經元的反應，讓他做出這樣的動作。」為什麼？\n讓我們從一個大家都有的共識開始討論吧。當你腳趾頭撞到桌腳時，你從不會懷疑自己痛苦而流漏出的猙獰表情是假的。因為你能確切地感受到，自己的感受。當你看到別人受苦時，因為別人跟你擁有一樣的機制，你能得到一個結論：那個人也有一個內在正在痛苦。\n然而你不能肯定，畢竟你不能親身經歷到他人的痛苦，你只能從自己的經驗，來揣測他人也有相同的經驗。從科學的角度觀測，一個人經歷痛苦，過程就是一連串的電磁訊號，發生在大腦，與有關的肌肉間。雖然科學還無法解讀這些電磁訊號，但已經可以確定，這些訊號與當下那個人的內在經驗綁在一起。\n如果一個人痛苦時身體運作的機制就是單純的物理現象。即便我們對這個機制並非全然了解，卻可以確定，這個機制大概不需要什麼額外的超自然元素摻雜在裡面。\n如果你願意承認單純的物理現象能夠產生豐富、複雜的內在經驗，那麼你就不能確定那些只存在遊戲裡面，運作在電腦處理器中的遊戲角色，當他們流露出痛苦、喜悅的表情、動作時，是否有個內在相映。或許你會聲稱，從程式的邏輯來看，確實沒有個內在經驗，程式僅是死板板的用一些變數，並做判斷，來產生那個角色的行為。\n你大概會是對的，用單純程式碼構築的遊戲角色，大概沒有複雜的到會產生內在經驗。但如果使用AI或是機器學習來產生遊戲角色的行為時，他們的行為將變得更加豐富，而最終，沒有人能理解內部程式運作的邏輯為何。如果他們的行為能與人相彷，只不過存在於遊戲中，他們會有內在經驗嗎？他們的痛苦會真的有個「靈魂」承受嗎？\n或許下次你在玩遊戲的時候，板機扣下之前猶豫一下，或者跟對面和解，說不定遊戲角色真的會感謝你（但不一定會表現出來）。\n","permalink":"https://fulva1230.github.io/daggy/posts/does-it-hurt/","summary":"在射擊遊戲中，當你奮力殺敵的時候，敵人痛苦的倒下，你是否想過一個問題，當他們倒下時，會不會痛？聽起來是個很奇怪的問題對吧，你肯定認為敵人會痛苦的倒下，是因為有人寫了程式，計算敵人受到的傷害，當生命值歸零的那個瞬間，便播放痛苦倒下的動畫，當然，動畫是人預先畫出來的。\n因為程式內部的計算是生硬的生命值加減，所以在遊戲中敵人痛苦的倒下只是個假象，實際上沒有一個意識，會經歷痛苦，並隨著你殺敵而消逝。但當你看著某個活生生的人，不管為了什麼原因，露出痛苦的表情時，你卻不會告訴自己說：「那個人只是因為大腦內電磁波、神經元的反應，讓他做出這樣的動作。」為什麼？\n讓我們從一個大家都有的共識開始討論吧。當你腳趾頭撞到桌腳時，你從不會懷疑自己痛苦而流漏出的猙獰表情是假的。因為你能確切地感受到，自己的感受。當你看到別人受苦時，因為別人跟你擁有一樣的機制，你能得到一個結論：那個人也有一個內在正在痛苦。\n然而你不能肯定，畢竟你不能親身經歷到他人的痛苦，你只能從自己的經驗，來揣測他人也有相同的經驗。從科學的角度觀測，一個人經歷痛苦，過程就是一連串的電磁訊號，發生在大腦，與有關的肌肉間。雖然科學還無法解讀這些電磁訊號，但已經可以確定，這些訊號與當下那個人的內在經驗綁在一起。\n如果一個人痛苦時身體運作的機制就是單純的物理現象。即便我們對這個機制並非全然了解，卻可以確定，這個機制大概不需要什麼額外的超自然元素摻雜在裡面。\n如果你願意承認單純的物理現象能夠產生豐富、複雜的內在經驗，那麼你就不能確定那些只存在遊戲裡面，運作在電腦處理器中的遊戲角色，當他們流露出痛苦、喜悅的表情、動作時，是否有個內在相映。或許你會聲稱，從程式的邏輯來看，確實沒有個內在經驗，程式僅是死板板的用一些變數，並做判斷，來產生那個角色的行為。\n你大概會是對的，用單純程式碼構築的遊戲角色，大概沒有複雜的到會產生內在經驗。但如果使用AI或是機器學習來產生遊戲角色的行為時，他們的行為將變得更加豐富，而最終，沒有人能理解內部程式運作的邏輯為何。如果他們的行為能與人相彷，只不過存在於遊戲中，他們會有內在經驗嗎？他們的痛苦會真的有個「靈魂」承受嗎？\n或許下次你在玩遊戲的時候，板機扣下之前猶豫一下，或者跟對面和解，說不定遊戲角色真的會感謝你（但不一定會表現出來）。","title":"Does It Hurt?"},{"content":"你曾否想過，整個宇宙不過是某個更高等文明創造的模擬器，而我們不過是那台模擬器中的程式碼，冷冰冰的從輸入計算出輸出，不斷地重複同樣的步驟，周而復始，毫無意義。\n如果我們不過是寫死的程式碼，那就好像某處有一本有關這個世界的劇本，所有理應發生的事都已寫在裡面，我們就是電影裡的演員，按著劇本分毫不差的說出台詞。很多我們熟悉的概念將僅是幻覺。\n但你肯定不會這麼覺得，誰願意相信自己僅是一團複雜的計算，而不是有情感，有創造力，有美感的一個人呢？但用電腦逐行計算，產生的行為就不可能擁有創造力、有認知呢？假設運算能力足夠，程式設計得夠複雜、精巧，要表現出一個人複雜的外在與內在並非不可能，只是很難罷了。\n所以我們表現出有靈魂的複雜樣貌這項事實，無法拿來反證宇宙只是模擬器的假設，因為我們可以是電腦，只是夠精巧，寫在我們裡面的程式夠複雜，讓我們以為自己超越「決定性」般的計算。\n如果連像人的行為這般複雜，難以理解的現象都能用模擬出來，那這世界上還有什麼是不能模擬的？或許沒有，但這不代表這世界就是模擬出來的。\n模擬的宇宙說不定可以成為費米悖論的答案，假設有個高等文明創造了這個宇宙，目的是為了觀察另一個文明從無到有，那麼我們肯定就是他們觀察的對象。再者，他們肯定會觀察宇宙中第一個出現的文明，代表除了我們宇宙中再無其他文明。或許我們該擔心一下，他們打算觀察到什麼時候，然後決定重來一遍。\n宇宙很大，人類的野心也是。我們想探究一切，所以提出大膽的假設，並嘗試駁斥或證實，無論如何，都必須保持開放的心態，接受所有的可能。\n","permalink":"https://fulva1230.github.io/daggy/posts/the-simulated-universe/","summary":"你曾否想過，整個宇宙不過是某個更高等文明創造的模擬器，而我們不過是那台模擬器中的程式碼，冷冰冰的從輸入計算出輸出，不斷地重複同樣的步驟，周而復始，毫無意義。\n如果我們不過是寫死的程式碼，那就好像某處有一本有關這個世界的劇本，所有理應發生的事都已寫在裡面，我們就是電影裡的演員，按著劇本分毫不差的說出台詞。很多我們熟悉的概念將僅是幻覺。\n但你肯定不會這麼覺得，誰願意相信自己僅是一團複雜的計算，而不是有情感，有創造力，有美感的一個人呢？但用電腦逐行計算，產生的行為就不可能擁有創造力、有認知呢？假設運算能力足夠，程式設計得夠複雜、精巧，要表現出一個人複雜的外在與內在並非不可能，只是很難罷了。\n所以我們表現出有靈魂的複雜樣貌這項事實，無法拿來反證宇宙只是模擬器的假設，因為我們可以是電腦，只是夠精巧，寫在我們裡面的程式夠複雜，讓我們以為自己超越「決定性」般的計算。\n如果連像人的行為這般複雜，難以理解的現象都能用模擬出來，那這世界上還有什麼是不能模擬的？或許沒有，但這不代表這世界就是模擬出來的。\n模擬的宇宙說不定可以成為費米悖論的答案，假設有個高等文明創造了這個宇宙，目的是為了觀察另一個文明從無到有，那麼我們肯定就是他們觀察的對象。再者，他們肯定會觀察宇宙中第一個出現的文明，代表除了我們宇宙中再無其他文明。或許我們該擔心一下，他們打算觀察到什麼時候，然後決定重來一遍。\n宇宙很大，人類的野心也是。我們想探究一切，所以提出大膽的假設，並嘗試駁斥或證實，無論如何，都必須保持開放的心態，接受所有的可能。","title":"The Simulated Universe"}]